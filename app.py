import os
import streamlit as st
from langchain.document_loaders import (
    PyPDFLoader, TextLoader, CSVLoader, UnstructuredWordDocumentLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_groq import ChatGroq
from langchain.prompts import PromptTemplate

from langchain.chains.question_answering import load_qa_chain
from langchain.chains import RetrievalQA

# === CONFIG ===
GROQ_API_KEY = "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"  # üîë ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô API Key ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
MODEL_NAME = "llama3-70b-8192"  # ‡∏´‡∏£‡∏∑‡∏≠ "llama3-8b-8192" ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Groq


#MODEL_NAME = "mixtral-8x7b-32768"   # ‚úÖ ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ
#MODEL_NAME = "llama3-8b-8192"       # ‚úÖ ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ
#MODEL_NAME = "llama3-70b-8192"      # ‚úÖ ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ
#MODEL_NAME = "gemma-7b-it"          # ‚úÖ ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ


# === Load documents ===
def load_documents():
    all_docs = []
    folder_path = "docs"
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)

        if filename.endswith(".pdf"):
            loader = PyPDFLoader(file_path)
        elif filename.endswith(".docx"):
            loader = UnstructuredWordDocumentLoader(file_path)
        elif filename.endswith(".txt"):
            loader = TextLoader(file_path)
        elif filename.endswith(".csv"):
            loader = CSVLoader(file_path)
        else:
            continue

        all_docs.extend(loader.load())
    return all_docs

# === Split documents ===
def split_documents(docs):
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    return splitter.split_documents(docs)

# === Build vectorstore with FAISS ===
def build_vectorstore(chunks):
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    vectordb = FAISS.from_documents(chunks, embedding=embeddings)
    return vectordb

# === Create QA Chain with Thai Prompt ===
def create_qa_chain(vectordb):
    llm = ChatGroq(
        temperature=0,
        groq_api_key="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",
        model_name="llama3-70b-8192"
    )
    
    prompt_template = """
    ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ ‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ:

    ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}
    ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:
    """

    prompt = PromptTemplate(template=prompt_template, input_variables=["question"])

    combine_chain = load_qa_chain(llm, chain_type="stuff", prompt=prompt)

    chain = RetrievalQA(
        retriever=vectordb.as_retriever(),
        combine_documents_chain=combine_chain
    )
    return chain


# === Main Streamlit App ===
def main():
    st.set_page_config(page_title="RAG Chatbot", layout="wide")
    st.title("üí¨ Chatbot ‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥ ‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏• ‡∏û.‡∏®. 2562")

    # ‡πÄ‡∏Å‡πá‡∏ö‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°-‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    # ‡∏õ‡∏∏‡πà‡∏°‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
    if st.button("üîÅ ‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤"):
        st.session_state.chat_history = []

    with st.spinner("üìö ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•..."):
        docs = load_documents()
        chunks = split_documents(docs)
        vectordb = build_vectorstore(chunks)
        qa_chain = create_qa_chain(vectordb)

    # ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
    query = st.text_input("üì• ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì:", placeholder="‡πÄ‡∏ä‡πà‡∏ô ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?")
    if query:
        with st.spinner("üß† ‡∏Ñ‡∏¥‡∏î‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö..."):
            answer = qa_chain.run(query)
            st.session_state.chat_history.append((query, answer))

    # ‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ (‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á)
    if st.session_state.chat_history:
        st.markdown("### üóÇÔ∏è ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤")
        with st.container():
            st.markdown(
                "<div style='max-width: 800px; margin-left: auto; margin-right: auto;'>",
                unsafe_allow_html=True
            )
            for i, (q, a) in enumerate(reversed(st.session_state.chat_history), 1):
                st.markdown(f"**{i}. ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:** {q}")
                st.markdown(f"üëâ **‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:** {a}")
                st.markdown("---")
            st.markdown("</div>", unsafe_allow_html=True)

if __name__ == "__main__":
    main()
